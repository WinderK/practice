{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outlook Humidity    Wind PlayTennis\n",
      "11  Cloudy     High  Strong        Yes\n",
      "2   Cloudy     High    Weak        Yes\n",
      "6   Cloudy   Normal  Strong        Yes\n",
      "12  Cloudy   Normal    Weak        Yes\n",
      "13   Rainy     High  Strong         No\n",
      "3    Rainy     High    Weak        Yes\n",
      "5    Rainy   Normal  Strong         No\n",
      "4    Rainy   Normal    Weak        Yes\n",
      "9    Rainy   Normal    Weak        Yes\n",
      "1    Sunny     High  Strong         No\n",
      "0    Sunny     High    Weak         No\n",
      "7    Sunny     High    Weak         No\n",
      "10   Sunny   Normal  Strong        Yes\n",
      "8    Sunny   Normal    Weak        Yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Outlook', 'Humidity', 'Wind', 'PlayTennis'],\n",
       " [{0: 'Cloudy', 1: 'Rainy', 2: 'Sunny'},\n",
       "  {0: 'High', 1: 'Normal'},\n",
       "  {0: 'Strong', 1: 'Weak'},\n",
       "  {0: 'No', 1: 'Yes'}])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Cloudy', 'Rainy', 'Rainy', 'Rainy', 'Cloudy', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Cloudy', 'Cloudy', 'Rainy'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.sort_values(by=['Outlook', 'Humidity', 'Wind']))\n",
    "df_copy = df.copy()\n",
    "mapping_dict_list = []\n",
    "# turn categories labels into int [0, 1, 2, ...]\n",
    "for c in df_copy.columns:\n",
    "    mapping_dict = df_copy[c].astype('category').cat.categories.tolist()\n",
    "    mapping_dict_list.append(dict(enumerate(mapping_dict)))\n",
    "    \n",
    "    # turn string labels into int\n",
    "    df_copy[c] = df_copy[c].astype('category').cat.codes\n",
    "\n",
    "X = df_copy.iloc[:, :-1].values\n",
    "y = df_copy.iloc[:, -1].values\n",
    "feature_names = df_copy.columns.tolist()\n",
    "feature_names, mapping_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(X, y, index: int):\n",
    "    X_i = X[:, index]\n",
    "    x_values = np.unique(X_i)\n",
    "    subsets = {}\n",
    "    for v in x_values:\n",
    "        selected = X_i == v\n",
    "        subsets[v] = (X[selected], y[selected])\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "\n",
    "def compute_entropy(y, binary=False):\n",
    "    eps = 1e-30\n",
    "    \n",
    "    def info(p):\n",
    "        return - p * np.log2(p + eps)\n",
    "    \n",
    "    if binary:\n",
    "        assert hasattr(y, \"__iter__\"), \"input in binary mode must be iterable!\"\n",
    "        \n",
    "        p = np.bincount(y) / len(y)  # np.bincount(y) counts of occurrences for each value from 0 to max(y)\n",
    "        infos = info(p) + info(1-p)\n",
    "        \n",
    "        index = infos.argmax()\n",
    "        max_info = infos[index]\n",
    "        return max_info, index\n",
    "    \n",
    "    if hasattr(y, \"__iter__\"):\n",
    "        p = np.bincount(y) / len(y)\n",
    "        return np.sum(info(p))\n",
    "    \n",
    "    else:\n",
    "        assert 0 <= y <= 1, \"p must satisfy 0 <= p <= 1\"\n",
    "        return info(y) + info(1-y)\n",
    "\n",
    "\n",
    "def compute_gini(y, binary=False):\n",
    "    if binary:\n",
    "        assert hasattr(y, \"__iter__\"), \"input in binary mode must be iterable!\"\n",
    "        \n",
    "        p = np.bincount(y) / len(y)  # np.bincount(y) counts of occurrences for each value from 0 to max(y)\n",
    "        ginis = 1 - p**2 - (1-p)**2\n",
    "        \n",
    "        index = ginis.argmax()\n",
    "        max_gini = ginis[index]\n",
    "        return max_gini, index\n",
    "\n",
    "    if hasattr(y, \"__iter__\"):\n",
    "        p = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(p**2)\n",
    "    \n",
    "    else:\n",
    "        assert 0 <= y <= 1, \"p must satisfy 0 <= p <= 1\"\n",
    "        return 1 - y**2 - (1-y)**2\n",
    "\n",
    "\n",
    "def split_gain(X, y, features=None, criterion=\"entropy\"):\n",
    "    '''\n",
    "    For all features in X, compute information gain. \n",
    "    X: np.array(n, k)\n",
    "    y: np.array(n)\n",
    "    features: int list, e.g. [0, 2]\n",
    "    '''\n",
    "    total_gain = None\n",
    "    if criterion == \"entropy\":\n",
    "        total_gain = compute_entropy(y)\n",
    "    elif criterion == \"gini\":\n",
    "        total_gain = 0\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if features is None:\n",
    "        features = range(X.shape[1])\n",
    "    \n",
    "    gains = {}\n",
    "    for i in features:\n",
    "        X_i = X[:, i]\n",
    "        n = len(X_i)\n",
    "        unique_x = np.unique(X_i)\n",
    "        gain_i = total_gain\n",
    "        \n",
    "        for x_label in unique_x:\n",
    "            y_sub = y[X_i == x_label]\n",
    "            \n",
    "            if criterion == \"entropy\":\n",
    "                # Gain_split = Gain_all - sum(|y_i|/|Y| * gain_i)\n",
    "                gain_i_sub = compute_entropy(y_sub)\n",
    "                gain_i -= len(y_sub) / n * gain_i_sub\n",
    "            elif criterion == \"gini\":\n",
    "                # Gini_split = sum(|y_i|/|Y| * gini_i)\n",
    "                gain_i_sub = compute_gini(y_sub)\n",
    "                gain_i += len(y_sub) / n * gain_i_sub\n",
    "            \n",
    "        \n",
    "        gains[i] = gain_i\n",
    "    \n",
    "    return gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tree:\n",
      " Outlook?\n",
      "    Cloudy: _Yes_\n",
      "    Rainy: Wind?\n",
      "        Strong: _No_\n",
      "        Weak: _Yes_\n",
      "    Sunny: Humidity?\n",
      "        High: _No_\n",
      "        Normal: _Yes_ \n",
      "\n",
      "Pruned tree:\n",
      " Outlook?\n",
      "    Cloudy: _Yes_\n",
      "    Rainy: _Yes_\n",
      "    Sunny: _No_ \n",
      "\n",
      "split gains: {0: 0.24674981977443933, 1: 0.15183550136234164, 2: 0.04812703040826949}, \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. compute information entropy\n",
    "2. compute information gain\n",
    "3. select best feature\n",
    "4. recurrently construct decision tree\n",
    "'''\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    feature: Rainy? Child: {Yes: No playing; No: Node(Sunny? ...)}\n",
    "    '''\n",
    "    def __init__(self, feature=None, label=None, child=None):\n",
    "        self.feature = feature  # -1 iff leaf node\n",
    "        self.child = child if child is not None else {}  # empty iff leaf node\n",
    "        self.label = label  # \"Yes\" or \"No\" iff leaf node; None if not leaf node\n",
    "    \n",
    "    # recurrently print tree\n",
    "    def __repr__(self, indent=\"\", depth=0, feature_names=None, mapping_dict_list=None):\n",
    "        if not self.child:\n",
    "            label = mapping_dict_list[self.feature][self.label]\n",
    "            return indent + f\"_{label}_\"\n",
    "        else:\n",
    "            feature = feature_names[self.feature]\n",
    "            result = indent + depth*4 * \" \" + f\"{feature}?\\n\"\n",
    "            for v, child in self.child.items():\n",
    "                value = mapping_dict_list[self.feature][v]\n",
    "                result += (depth+1)*4 * \" \" + f\"{value}: \"\n",
    "                result += child.__repr__(indent, depth+1, feature_names, mapping_dict_list) + \"\\n\"\n",
    "            result = result.strip(\"\\n\")\n",
    "            return result.strip()\n",
    "\n",
    "\n",
    "# https://zhuanlan.zhihu.com/p/197476119\n",
    "class ID3:\n",
    "    def __init__(self, criterion='entropy'):\n",
    "        if criterion not in [\"entropy\", \"gini\"]:\n",
    "            raise ValueError(f'criterion must be one of [\"entropy\", \"gini\"]')\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.feature_names = None\n",
    "        self.mapping_dict_list = None\n",
    "        self.root = None\n",
    "    \n",
    "    def make_tree(self, df: pd.DataFrame, max_depth=None, min_samples_leaf=1, alpha=0.5):\n",
    "        '''\n",
    "        default: y is at the last column of 'df'. fit() uses this. \n",
    "        '''\n",
    "        df = df.copy()  # to prevent astype() alter original table\n",
    "        self.feature_names = df.columns.tolist()\n",
    "        \n",
    "        mapping_dict_list = []\n",
    "        inv_mapping_list = []\n",
    "        # turn categories labels into int [0, 1, 2, ...]\n",
    "        for c in df.columns:\n",
    "            mapping_dict = df[c].astype('category').cat.categories.tolist()\n",
    "            d = dict(enumerate(mapping_dict))\n",
    "            mapping_dict_list.append(d)\n",
    "            \n",
    "            inv_d = {v: k for (k, v) in d.items()}\n",
    "            inv_mapping_list.append(inv_d)\n",
    "            \n",
    "            # turn string labels into int\n",
    "            df[c] = df[c].astype('category').cat.codes\n",
    "\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "        self.mapping_dict_list = mapping_dict_list\n",
    "        # predict() uses this\n",
    "        self.inv_mapping_list = inv_mapping_list\n",
    "        \n",
    "        self.root = self.fit(X, y)\n",
    "        \n",
    "        self.pessimistic_error_pruning(\n",
    "            X, y, max_depth, min_samples_leaf, alpha\n",
    "            )\n",
    "    \n",
    "    # recurrently called\n",
    "    def fit(self, X, y, features=[]):\n",
    "        '''\n",
    "        e.g. make_tree(X_sub, y_sub, [0, 2])\n",
    "        As for now, columns of X are not reduced when doing recursion. Select() only selects rows. \n",
    "        '''\n",
    "        ys = np.unique(y)\n",
    "        if len(ys) == 1:\n",
    "            # default: y is at the last column of 'df'\n",
    "            return Node(feature=-1, label=ys[0])  # leaf node\n",
    "        \n",
    "        if features == []:\n",
    "            features = list(range(X.shape[1]))\n",
    "        \n",
    "        # compute score for each subset\n",
    "        gains = split_gain(X, y, features, self.criterion)  # e.g. {0: 1.1, 2: 0.5}\n",
    "        \n",
    "        # choose the current split feature\n",
    "        if self.criterion == \"entropy\":\n",
    "            best_feature = max(gains.items(), key=lambda x: x[1])[0]  # e.g. (0, 1.1)[0] = 0\n",
    "        elif self.criterion == \"gini\":\n",
    "            best_feature = min(gains.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        node = Node(feature=best_feature)\n",
    "        subsets = select_data(X, y, best_feature)\n",
    "        \n",
    "        for feature, (X_sub, y_sub) in subsets.items():\n",
    "            # feature is an integer\n",
    "            # leaf node\n",
    "            new_features = features.copy()\n",
    "            new_features.remove(best_feature)\n",
    "            assert new_features is not None, f\"'new_features' is None\"\n",
    "            node.child[feature] = self.fit(X_sub, y_sub, new_features)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    # naive version\n",
    "    def pessimistic_error_pruning(\n",
    "        self, X, y, max_depth=None, min_samples_leaf=1, alpha=0.5\n",
    "        ):\n",
    "        '''\n",
    "        Estimate each node with train set (to judge whether to prune or not). \n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training features.\n",
    "        - y: Training labels.\n",
    "        - max_depth: Maximum depth of the tree (optional).\n",
    "        - min_samples_leaf: Minimum number of samples required to be at a leaf node (optional).\n",
    "        - alpha: Pessimistic error adjustment factor (optional).\n",
    "        \n",
    "        https://www.bilibili.com/read/cv11072586/\n",
    "        https://www.cnblogs.com/zxz666/p/15364751.html\n",
    "        https://zhuanlan.zhihu.com/p/368912628\n",
    "        \n",
    "        When e' <= e + se(e), prune. Pruning can reduce the upper bound of error rate at the node. \n",
    "        When e' >  e + se(e), do not prune. That will make things worse. \n",
    "        \n",
    "        e = original misclassified sample number\n",
    "        se = sqrt(np(1-p)), where p = e/n. Binomial distribution. \n",
    "        e' = positive / negative sample number\n",
    "        '''\n",
    "        def calculate_errors(node, X, y):\n",
    "            if node.label is not None:\n",
    "                # leaf node: e/n\n",
    "                return len(y != node.label)\n",
    "            else:\n",
    "                errors = 0\n",
    "                # TODO!: can we reduce time?\n",
    "                for value, child in node.child.items():\n",
    "                    mask = X[:, node.feature] == value\n",
    "                    errors = calculate_errors(child, X[mask], y[mask])\n",
    "            return errors\n",
    "        \n",
    "        # TODO: store num_leaf for each node\n",
    "        # TODO: use update_num_leaf() after pruning\n",
    "        def count_leaf_nodes(node):\n",
    "            if node.label is not None:  # leaf node\n",
    "                return 1\n",
    "            else:\n",
    "                return sum(count_leaf_nodes(child) for child in node.child.values())\n",
    "        \n",
    "        def compute_pessimistic_error(node, X, y, alpha):\n",
    "            e = calculate_errors(node, X, y)\n",
    "            num_leaf = count_leaf_nodes(node)\n",
    "            return e + alpha * num_leaf\n",
    "        \n",
    "        def prune_node(node, X, y, alpha=0.5, depth=0):\n",
    "            '''\n",
    "            Recurrently called to prune nodes.\n",
    "            (X, y): subset of dataset\n",
    "            alpha: penalty coefficient, default=0.5\n",
    "            \n",
    "            steps:\n",
    "            1. check whether to prune or not\n",
    "                leaf node? node too deep? samples too few? reduces error rate?\n",
    "            2. do pruning\n",
    "            '''\n",
    "            def prune(node: Node):\n",
    "                node.feature = -1\n",
    "                node.child = {}\n",
    "                node.label = np.bincount(y).argmax()\n",
    "            \n",
    "            # 1. check whether to prune or not\n",
    "            # leaf node\n",
    "            if node.label is not None:  \n",
    "                return\n",
    "            \n",
    "            # node is too deep\n",
    "            if max_depth is not None and depth > max_depth:\n",
    "                prune(node)\n",
    "                return\n",
    "            \n",
    "            # samples are too few\n",
    "            if X.shape[1] < min_samples_leaf:\n",
    "                prune(node)\n",
    "                return\n",
    "            \n",
    "            # e' <= e + se(e)\n",
    "            n = len(y)\n",
    "            e = compute_pessimistic_error(node, X, y, alpha)\n",
    "            # se = sqrt(np(1-p)) = sqrt(e(n-e)/n)\n",
    "            se = np.sqrt(e*(n-e)/n)\n",
    "            \n",
    "            # pruned node: majority voting\n",
    "            node_pruned = Node(label=np.bincount(y).argmax())\n",
    "            e_pruned = compute_pessimistic_error(node_pruned, X, y, alpha)\n",
    "            \n",
    "            if e_pruned <= e + se:\n",
    "                prune(node)\n",
    "            else:\n",
    "                for value, child in node.child.items():\n",
    "                    mask = X[:, node.feature] == value\n",
    "                    prune_node(child, X[mask], y[mask], alpha, depth+1)\n",
    "        \n",
    "        # -----------------------------------------------------------------------\n",
    "        prune_node(self.root, X, y, alpha, depth=1)\n",
    "\n",
    "    def predict(self, x_str):\n",
    "        if isinstance(x_str[0], str) or not hasattr(x_str[0], \"__iter__\"):\n",
    "            n = len(x_str)\n",
    "            assert n == len(self.feature_names) - 1, f\"{n} != {len(self.feature_names) - 1}\"\n",
    "            \n",
    "            X = [self.inv_mapping_list[i][x_str[i]] for i in range(n)]  # str -> int\n",
    "            current = self.root\n",
    "            while current.child:\n",
    "                assert current.label is None  # test if I wrote codes right\n",
    "                \n",
    "                feature = current.feature\n",
    "                current = current.child[X[feature]]\n",
    "            \n",
    "            return self.mapping_dict_list[current.feature][current.label]\n",
    "        \n",
    "        else:\n",
    "            results = []\n",
    "            for X in x_str:\n",
    "                results.append(self.predict(X))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.root.__repr__(feature_names=self.feature_names, \n",
    "                                mapping_dict_list=self.mapping_dict_list)\n",
    "\n",
    "\n",
    "\n",
    "criterion = 'entropy'\n",
    "tree = ID3(criterion=criterion)\n",
    "tree.make_tree(df)\n",
    "print('Original tree:\\n', tree, '\\n')\n",
    "\n",
    "tree.make_tree(df, max_depth=1, min_samples_leaf=1)\n",
    "print('Pruned tree:\\n', tree, '\\n')\n",
    "\n",
    "gains = split_gain(X, y, criterion=criterion)\n",
    "print(f\"split gains: {gains}, \\n\")\n",
    "\n",
    "tree.predict(['Sunny', 'Normal', 'Weak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlook?\n",
      "    Cloudy: _Yes_\n",
      "    Rainy: Wind?\n",
      "        Strong: _No_\n",
      "        Weak: _Yes_\n",
      "    Sunny: Humidity?\n",
      "        High: _No_\n",
      "        Normal: _Yes_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = ID3(criterion='gini')\n",
    "tree.make_tree(df)\n",
    "print(tree)\n",
    "tree.predict(['Sunny', 'Normal', 'Weak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_entropy(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.44217935649972373"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(0.6)*0.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
